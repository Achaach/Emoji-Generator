{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51815eb6",
   "metadata": {},
   "source": [
    "# Emoji Generator From Emotion Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "780ae037",
   "metadata": {},
   "source": [
    "Emojify from video is a project that involves using computer vision and machine learning techniques to detect and track facial expressions and emotions in a video and then overlay corresponding emojis onto the faces in real-time.\n",
    "\n",
    "The project involves several steps, including:\n",
    "\n",
    "1. Face detection and tracking: \n",
    "This involves using a computer vision algorithm to detect and track faces in a video. Once a face is detected, the algorithm will track it as it moves throughout the video. We are using CNN to do this facial detection here.\n",
    "2. Emotion detection: \n",
    "This involves using a machine learning algorithm to analyze the facial expressions of the person in the video and determine their emotional state. There are several approaches to this, including using facial landmarks, extracting features from the face, and analyzing the movement of the face muscles.\n",
    "3. Emojification: \n",
    "Once the emotional state of the person in the video has been determined, the appropriate emoji can be overlaid onto the video so that we can see what emotion emoji they are.\n",
    "\n",
    "Overall, the goal of the project is to create a fun and engaging way to visualize emotions in a video, and potentially have applications in areas such as video conferencing, education, and entertainment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb5a2f0",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5c483c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from tensorflow.keras.models import Sequential \n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.preprocessing.image import ImageDataGenerator##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8eb902cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 28709 images belonging to 7 classes.\n",
      "Found 7178 images belonging to 7 classes.\n"
     ]
    }
   ],
   "source": [
    "train_dir = 'data/train'\n",
    "val_dir = 'data/test'\n",
    "train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "val_datagen = ImageDataGenerator(rescale=1./255)\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        train_dir,\n",
    "        target_size=(48,48),\n",
    "        batch_size=64,\n",
    "        color_mode=\"grayscale\",\n",
    "        class_mode='categorical')\n",
    "validation_generator = val_datagen.flow_from_directory(\n",
    "        val_dir,\n",
    "        target_size=(48,48),\n",
    "        batch_size=64,\n",
    "        color_mode=\"grayscale\",\n",
    "        class_mode='categorical')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f89ae9",
   "metadata": {},
   "source": [
    "Now we can build the convolution network architecture to train the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a6e492f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-27 16:33:04.263696: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "emotion_model = Sequential()\n",
    "emotion_model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(48,48,1)))\n",
    "emotion_model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
    "emotion_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "emotion_model.add(Dropout(0.25))\n",
    "emotion_model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n",
    "emotion_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "emotion_model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n",
    "emotion_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "emotion_model.add(Dropout(0.25))\n",
    "emotion_model.add(Flatten())\n",
    "emotion_model.add(Dense(1024, activation='relu'))\n",
    "emotion_model.add(Dropout(0.5))\n",
    "emotion_model.add(Dense(7, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d69a880a",
   "metadata": {},
   "source": [
    "Then we compile and train the model, after training, we save the model as `model.h5`.\n",
    "\n",
    "Note: For higher accuracy, we can change the epochs to be higher, here we set `epochs = 30` for our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a1e4c2f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/hp/m_3sfw2d3gnd52gvxwg38bk00000gn/T/ipykernel_33218/1130765543.py:4: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  emotion_model_info = emotion_model.fit_generator(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "448/448 [==============================] - 210s 466ms/step - loss: 1.7980 - accuracy: 0.2647 - val_loss: 1.6980 - val_accuracy: 0.3449\n",
      "Epoch 2/30\n",
      "448/448 [==============================] - 239s 533ms/step - loss: 1.6272 - accuracy: 0.3646 - val_loss: 1.5473 - val_accuracy: 0.4129\n",
      "Epoch 3/30\n",
      "448/448 [==============================] - 233s 520ms/step - loss: 1.5320 - accuracy: 0.4103 - val_loss: 1.4763 - val_accuracy: 0.4379\n",
      "Epoch 4/30\n",
      "448/448 [==============================] - 194s 432ms/step - loss: 1.4593 - accuracy: 0.4421 - val_loss: 1.3936 - val_accuracy: 0.4706\n",
      "Epoch 5/30\n",
      "448/448 [==============================] - 179s 398ms/step - loss: 1.3945 - accuracy: 0.4655 - val_loss: 1.3415 - val_accuracy: 0.4869\n",
      "Epoch 6/30\n",
      "448/448 [==============================] - 190s 425ms/step - loss: 1.3377 - accuracy: 0.4942 - val_loss: 1.2944 - val_accuracy: 0.5078\n",
      "Epoch 7/30\n",
      "448/448 [==============================] - 183s 408ms/step - loss: 1.2951 - accuracy: 0.5082 - val_loss: 1.2694 - val_accuracy: 0.5176\n",
      "Epoch 8/30\n",
      "448/448 [==============================] - 183s 409ms/step - loss: 1.2551 - accuracy: 0.5272 - val_loss: 1.2397 - val_accuracy: 0.5315\n",
      "Epoch 9/30\n",
      "448/448 [==============================] - 192s 428ms/step - loss: 1.2188 - accuracy: 0.5396 - val_loss: 1.2145 - val_accuracy: 0.5325\n",
      "Epoch 10/30\n",
      "448/448 [==============================] - 199s 444ms/step - loss: 1.1933 - accuracy: 0.5505 - val_loss: 1.2021 - val_accuracy: 0.5448\n",
      "Epoch 11/30\n",
      "448/448 [==============================] - 194s 433ms/step - loss: 1.1640 - accuracy: 0.5598 - val_loss: 1.1676 - val_accuracy: 0.5547\n",
      "Epoch 12/30\n",
      "448/448 [==============================] - 211s 470ms/step - loss: 1.1333 - accuracy: 0.5762 - val_loss: 1.1655 - val_accuracy: 0.5547\n",
      "Epoch 13/30\n",
      "448/448 [==============================] - 183s 409ms/step - loss: 1.1145 - accuracy: 0.5838 - val_loss: 1.1427 - val_accuracy: 0.5709\n",
      "Epoch 14/30\n",
      "448/448 [==============================] - 182s 406ms/step - loss: 1.0872 - accuracy: 0.5929 - val_loss: 1.1323 - val_accuracy: 0.5717\n",
      "Epoch 15/30\n",
      "448/448 [==============================] - 180s 401ms/step - loss: 1.0601 - accuracy: 0.6060 - val_loss: 1.1231 - val_accuracy: 0.5787\n",
      "Epoch 16/30\n",
      "448/448 [==============================] - 180s 400ms/step - loss: 1.0402 - accuracy: 0.6125 - val_loss: 1.1070 - val_accuracy: 0.5840\n",
      "Epoch 17/30\n",
      "448/448 [==============================] - 172s 384ms/step - loss: 1.0183 - accuracy: 0.6244 - val_loss: 1.1018 - val_accuracy: 0.5852\n",
      "Epoch 18/30\n",
      "448/448 [==============================] - 175s 391ms/step - loss: 0.9936 - accuracy: 0.6310 - val_loss: 1.1133 - val_accuracy: 0.5830\n",
      "Epoch 19/30\n",
      "448/448 [==============================] - 204s 454ms/step - loss: 0.9699 - accuracy: 0.6411 - val_loss: 1.0854 - val_accuracy: 0.5940\n",
      "Epoch 20/30\n",
      "448/448 [==============================] - 191s 427ms/step - loss: 0.9494 - accuracy: 0.6481 - val_loss: 1.0807 - val_accuracy: 0.6017\n",
      "Epoch 21/30\n",
      "448/448 [==============================] - 173s 386ms/step - loss: 0.9219 - accuracy: 0.6595 - val_loss: 1.0817 - val_accuracy: 0.6021\n",
      "Epoch 22/30\n",
      "448/448 [==============================] - 176s 392ms/step - loss: 0.9064 - accuracy: 0.6644 - val_loss: 1.0784 - val_accuracy: 0.5996\n",
      "Epoch 23/30\n",
      "448/448 [==============================] - 197s 440ms/step - loss: 0.8876 - accuracy: 0.6750 - val_loss: 1.0724 - val_accuracy: 0.5993\n",
      "Epoch 24/30\n",
      "448/448 [==============================] - 205s 457ms/step - loss: 0.8610 - accuracy: 0.6851 - val_loss: 1.0792 - val_accuracy: 0.6035\n",
      "Epoch 25/30\n",
      "448/448 [==============================] - 209s 467ms/step - loss: 0.8422 - accuracy: 0.6925 - val_loss: 1.0791 - val_accuracy: 0.6064\n",
      "Epoch 26/30\n",
      "448/448 [==============================] - 225s 501ms/step - loss: 0.8157 - accuracy: 0.7019 - val_loss: 1.0616 - val_accuracy: 0.6108\n",
      "Epoch 27/30\n",
      "448/448 [==============================] - 251s 560ms/step - loss: 0.7988 - accuracy: 0.7080 - val_loss: 1.0613 - val_accuracy: 0.6165\n",
      "Epoch 28/30\n",
      "448/448 [==============================] - 218s 485ms/step - loss: 0.7664 - accuracy: 0.7209 - val_loss: 1.0627 - val_accuracy: 0.6159\n",
      "Epoch 29/30\n",
      "448/448 [==============================] - 233s 520ms/step - loss: 0.7549 - accuracy: 0.7256 - val_loss: 1.0723 - val_accuracy: 0.6143\n",
      "Epoch 30/30\n",
      "448/448 [==============================] - 228s 509ms/step - loss: 0.7273 - accuracy: 0.7361 - val_loss: 1.0827 - val_accuracy: 0.6134\n"
     ]
    }
   ],
   "source": [
    "#Compile and train the model:\n",
    "\n",
    "#emotion_model.compile(loss='categorical_crossentropy',optimizer=Adam(lr=0.0001, decay=1e-6),metrics=['accuracy'])\n",
    "#emotion_model_info = emotion_model.fit_generator(\n",
    "#        train_generator,\n",
    "#        steps_per_epoch=28709 // 64,\n",
    "#        epochs=30,\n",
    "#        validation_data=validation_generator,\n",
    "#        validation_steps=7178 // 64)\n",
    "\n",
    "#emotion_model.save_weights('model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e23e1a80",
   "metadata": {},
   "source": [
    "Now we can read the saved model from our file directory directly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aa7ed96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_model.load_weights('model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f42f945",
   "metadata": {},
   "source": [
    "After building above model, we can build an emoji generator to adjust emoji from emotion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "226bf3a1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 73ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [16]\u001b[0m, in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ret:\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m bounding_box \u001b[38;5;241m=\u001b[39m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCascadeClassifier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhaarcascades\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhaarcascade_frontalface_default.xml\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m gray_frame \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mcvtColor(frame, cv2\u001b[38;5;241m.\u001b[39mCOLOR_BGR2GRAY)\n\u001b[1;32m     14\u001b[0m num_faces \u001b[38;5;241m=\u001b[39m bounding_box\u001b[38;5;241m.\u001b[39mdetectMultiScale(gray_frame,scaleFactor\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.3\u001b[39m, minNeighbors\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "cv2.ocl.setUseOpenCL(False)\n",
    "\n",
    "emotion_dict = {0: \"Angry\", 1: \"Disgusted\", 2: \"Fearful\", 3: \"Happy\", 4: \"Neutral\", 5: \"Sad\", 6: \"Surprised\"}\n",
    "emoji_dist={0:\"emojis/angry.png\",2:\"emojis/disgusted.png\",2:\"emojis/fearful.png\",3:\"emojis/happy.png\",4:\"emojis/neutral.png\",5:\"emojis/sad.png\",6:\"emojis/surprised.png\"}\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    bounding_box = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "    gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    num_faces = bounding_box.detectMultiScale(gray_frame,scaleFactor=1.3, minNeighbors=5)\n",
    "\n",
    "    for (x, y, w, h) in num_faces:\n",
    "        cv2.rectangle(frame, (x, y-50), (x+w, y+h+10), (255, 0, 0), 2)\n",
    "        roi_gray_frame = gray_frame[y:y + h, x:x + w]\n",
    "        cropped_img = np.expand_dims(np.expand_dims(cv2.resize(roi_gray_frame, (48, 48)), -1), 0)\n",
    "        emotion_prediction = emotion_model.predict(cropped_img)\n",
    "        maxindex = int(np.argmax(emotion_prediction))\n",
    "        img_idx = emoji_dist[maxindex]\n",
    "        img = cv2.imread(img_idx)\n",
    "        img = cv2.resize(img, dsize=(300,300), interpolation=cv2.INTER_CUBIC)\n",
    "        img_height, img_width = img.shape[0], img.shape[1]\n",
    "        cv2.putText(frame, emotion_dict[maxindex], (x+20, y-60), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "        frame[0:300 , 0:300 ] = img\n",
    "\n",
    "    cv2.imshow('Video', cv2.resize(frame,(1200,860),interpolation = cv2.INTER_CUBIC))\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "        \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c833687e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
